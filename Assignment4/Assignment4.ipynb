{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Assignment 4 - Multilayer Perceptron to Recognize Handwritten Digits [12 Marks]</h2>\n",
    "<br>\n",
    "<b>Due Date:</b> December 03 at 23.59.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multilayer Perceptron [9 Marks]</h3>\n",
    "\n",
    "We will start by implementing the multilayer perceptron.\n",
    "\n",
    "1. [0 Marks] Read the code that is provided to you. You should start with the constructor, where the weights of the model are initialized. The constructor `__init__` is already fully implemented for you. Please read carefully the instructions using the dictionary `self.weights`. You will implement something similar with the dictionary `self.cache`. Then, read the method `train`, which is also fully implemented for you. It is in this method that the three methods you will need to implement are invoked. The method `train` first calls the forward propagation step (method `forward`), where the outputs of the model are computed. Then it calls the backward step (method `backward`), where the gradients of all weights are computed. Finally, it calls the update step (`update`), where we use the gradients to update the weights of the model.\n",
    "2. [2.5 Marks] Implement the method `forward`. In your implementation you should store in `self.cache` the `Zi` and `Ai` values computed in the forward pass (see the lecture notes for details).\n",
    "3. [4 Marks] Implement the `backward` method. In your implementation you should store in `self.cache` the `dW` and `dB` values with the gradients of all weights of the model. <b>You should assume the Cross Entropy loss</b>. This means that we don't multiply the pure error in the output layer by the derivative of the logistic function while computing $\\Delta^{(L-1)}$. We still need to use the derivative of the logistic function when computing the $\\Delta^{(i)}$ for the units in the hidden layers.\n",
    "4. [2.5 Marks] Implement the `update` method. In this method you will use the values of `dW` and `dB` to update the weights of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    Class implementing Multilayer Perceptron with sigmoid activation functions, which are used\n",
    "    both in the hidden layers and in the output layer of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims):\n",
    "        \"\"\"\n",
    "        When creating an MLP object we define the architecture of the model (number of layers and number\n",
    "        of units in each layer). The architecture is defined by a vector with a number of entries matching\n",
    "        the number of units in each layer.\n",
    "\n",
    "        For example, if dims = [784, 50, 20, 10], then the model has two hidden layers, with 50 and 20 neurons each,\n",
    "        and an output layer with 10 units. The input layer receives 784 values.\n",
    "\n",
    "        Since we are performing classification of handwritten digits, the input layer (784 units) and the output\n",
    "        layer (10 units) are fixed: the input layer has one value for each pixel in the image, while the output\n",
    "        layer has 10 units for each digit (0, 1, 2, ..., 9).\n",
    "\n",
    "        The data of the model will be stored in dictionaries where the key is given by the name of the value store.\n",
    "        In particular, we will have one dictionary called self.weights to store the weights and biases of the model;\n",
    "        we will also have a dictionary called self.cache where we will store the matrices Z, A, and Delta matrices\n",
    "        used in the backpropagation algorithm.\n",
    "\n",
    "        The constructor also initializes the set of weights the model uses. For example, if dims = [784, 50, 20, 10],\n",
    "        then the model has 3 sets of weights: one between the input layer and the first 50 units; another set of weights\n",
    "        between 50 and 20; and a last set of weights between 20 and 10. We initialize the weights with small random numbers.\n",
    "        The weights of the B vectors are initialized with zero.\n",
    "        \"\"\"\n",
    "        self.dims = dims\n",
    "        self.weights = {}\n",
    "        self.L = len(dims)\n",
    "\n",
    "        for i in range(1, len(dims)):\n",
    "            self.weights[f\"W{i-1}\"] = np.random.randn(dims[i], dims[i - 1]) * (\n",
    "                2 / np.sqrt(dims[i - 1])\n",
    "            )\n",
    "            self.weights[f\"B{i-1}\"] = np.zeros((dims[i], 1))\n",
    "\n",
    "    def derivative(self, A):\n",
    "        \"\"\"\n",
    "        Derivative of the logistic function\n",
    "        \"\"\"\n",
    "        return np.multiply(A, 1 - A)\n",
    "\n",
    "    def activation_function(self, Z):\n",
    "        \"\"\"\n",
    "        Logistic function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass. We initialize the self.cache dictionary with the vector representing the input layers, denoted A0.\n",
    "        The forward pass then computes Zi and Ai until reaching the output layer. The last matrix A is then returned.\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.cache[\"A0\"] = X\n",
    "\n",
    "        for i in range(1, self.L):\n",
    "            self.cache[f\"Z{i}\"] = (\n",
    "                np.dot(self.weights[f\"W{i - 1}\"], self.cache[f\"A{i - 1}\"])\n",
    "                + self.weights[f\"B{i - 1}\"]\n",
    "            )\n",
    "            self.cache[f\"A{i}\"] = self.activation_function(self.cache[f\"Z{i}\"])\n",
    "\n",
    "        return self.cache[f\"A{self.L - 1}\"]\n",
    "\n",
    "    def backward(self, Y):\n",
    "        \"\"\"\n",
    "        This function implements the backward step of the Backprop algorithm.\n",
    "\n",
    "        The deltas di and gradients dW and dB are stored in self.cache with the keys di, dWi, and dBi, respectively.\n",
    "        \"\"\"\n",
    "\n",
    "        self.cache[f\"d{self.L - 1}\"] = self.cache[f\"A{self.L - 1}\"] - Y\n",
    "\n",
    "        for i in reversed(range(0, self.L - 1)):\n",
    "            self.cache[f\"dW{i}\"] = np.dot(\n",
    "                self.cache[f\"d{i + 1}\"], self.cache[f\"A{i}\"].T\n",
    "            )\n",
    "            self.cache[f\"dB{i}\"] = self.cache[f\"d{i + 1}\"]\n",
    "\n",
    "            if i > 0:\n",
    "                self.cache[f\"d{i}\"] = np.dot(\n",
    "                    self.weights[f\"W{i}\"].T, self.cache[f\"d{i + 1}\"]\n",
    "                ) * self.derivative(self.cache[f\"A{i}\"])\n",
    "\n",
    "    def update(self, alpha):\n",
    "        \"\"\"\n",
    "        Function must be called after backward is invoked.\n",
    "\n",
    "        It will use the dWs and dBs stored in self.cache to update the weights self.weights of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        m = self.cache[f\"A{self.L - 1}\"].shape[1]\n",
    "\n",
    "        for i in range(0, self.L - 1):\n",
    "            self.weights[f\"W{i}\"] -= (alpha / m) * self.cache[f\"dW{i}\"]\n",
    "\n",
    "            self.weights[f\"B{i}\"] -= ((alpha / m) * self.cache[f\"dB{i}\"]).sum(\n",
    "                axis=1, keepdims=True\n",
    "            )\n",
    "\n",
    "    def train(self, X, Y, X_validation, Y_validation, alpha, steps):\n",
    "        # creating one-hot encoding for the labels of the images\n",
    "        Y_one_hot = np.zeros((10, X.shape[1]))\n",
    "        for index, value in enumerate(Y):\n",
    "            Y_one_hot[value][index] = 1\n",
    "\n",
    "        # performs a number of gradient descent steps\n",
    "        for i in range(0, steps):\n",
    "            # computes matrices A and store them in self.cache\n",
    "            self.forward(X)\n",
    "\n",
    "            # computes matrices dW and dB and store them in self.cache\n",
    "            self.backward(Y_one_hot)\n",
    "\n",
    "            # use the matrices dW and dB to update the weights W and B of the model\n",
    "            self.update(alpha)\n",
    "\n",
    "            # every 100 training steps we print the accuracies of the model on a set of training and validation data sets\n",
    "            if i % 100 == 0:\n",
    "                percentage_train = self.evaluate(X, Y)\n",
    "                percentage_validation = self.evaluate(X_validation, Y_validation)\n",
    "\n",
    "                print(\n",
    "                    \"Accuracy training set %.3f, Accuracy validation set %.3f \"\n",
    "                    % (percentage_train, percentage_validation)\n",
    "                )\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        \"\"\"\n",
    "        Receives a set of images stacked as column vectors in matrix X, their one-hot labels Y.\n",
    "\n",
    "        Returns the percentage of images that were correctly classified by the model.\n",
    "        \"\"\"\n",
    "        Y_hat = self.forward(X)\n",
    "        classified_correctly = test_correct = np.count_nonzero(\n",
    "            np.argmax(Y_hat, axis=0) == Y\n",
    "        )\n",
    "        return classified_correctly / X.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to test your implementation. We are following the scheme of stacking up input images as column vectors of `X`, as we did in Assignment 3 (see the lecture notes for Backpropagation for details).\n",
    "\n",
    "We will run backpropagation for $1000$ training iterations on a data set with $20000$ training images. During training we will also compute the accuracy of the model on a set of $10000$, which we call the validation set.\n",
    "\n",
    "<b>Disclaimer:</b> The outputs of the next cell won't be meaningful before you finish implementing the multilayer perceptron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy training set 0.106, Accuracy validation set 0.111 \n",
      "Accuracy training set 0.775, Accuracy validation set 0.766 \n",
      "Accuracy training set 0.855, Accuracy validation set 0.851 \n",
      "Accuracy training set 0.888, Accuracy validation set 0.882 \n",
      "Accuracy training set 0.904, Accuracy validation set 0.895 \n",
      "Accuracy training set 0.913, Accuracy validation set 0.904 \n",
      "Accuracy training set 0.921, Accuracy validation set 0.910 \n",
      "Accuracy training set 0.928, Accuracy validation set 0.917 \n",
      "Accuracy training set 0.934, Accuracy validation set 0.920 \n",
      "Accuracy training set 0.938, Accuracy validation set 0.924 \n",
      "Accuracy training set 0.942, Accuracy validation set 0.927 \n",
      "Accuracy training set 0.946, Accuracy validation set 0.930 \n",
      "Accuracy training set 0.949, Accuracy validation set 0.933 \n",
      "Accuracy training set 0.952, Accuracy validation set 0.935 \n",
      "Accuracy training set 0.955, Accuracy validation set 0.937 \n",
      "Accuracy training set 0.957, Accuracy validation set 0.938 \n",
      "Accuracy training set 0.959, Accuracy validation set 0.939 \n",
      "Accuracy training set 0.961, Accuracy validation set 0.940 \n",
      "Accuracy training set 0.963, Accuracy validation set 0.941 \n",
      "Accuracy training set 0.965, Accuracy validation set 0.942 \n",
      "Accuracy training set 0.967, Accuracy validation set 0.943 \n",
      "Accuracy training set 0.969, Accuracy validation set 0.944 \n",
      "Accuracy training set 0.971, Accuracy validation set 0.944 \n",
      "Accuracy training set 0.972, Accuracy validation set 0.946 \n",
      "Accuracy training set 0.973, Accuracy validation set 0.947 \n",
      "Accuracy training set 0.974, Accuracy validation set 0.947 \n",
      "Accuracy training set 0.975, Accuracy validation set 0.949 \n",
      "Accuracy training set 0.977, Accuracy validation set 0.949 \n",
      "Accuracy training set 0.978, Accuracy validation set 0.950 \n",
      "Accuracy training set 0.978, Accuracy validation set 0.950 \n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "m = 20000\n",
    "val = 10000\n",
    "images, labels = (x_train[0:m].reshape(m, 28 * 28) / 255, y_train[0:m])\n",
    "images = images.T\n",
    "\n",
    "images_validation, labels_validation = (\n",
    "    x_train[m : m + val].reshape(val, 28 * 28) / 255,\n",
    "    y_train[m : m + val],\n",
    ")\n",
    "images_validation = images_validation.T\n",
    "\n",
    "dims = [784, 50, 20, 10]\n",
    "mlp = MLP(dims)\n",
    "mlp.train(images, labels, images_validation, labels_validation, 0.5, 3000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question 1 [3 Marks]</h4>\n",
    "\n",
    "How do the results you obtained with the multilayer perceptron compare with the results you obtained with Logistic Regression using Cross Entropy loss in Assignment 3? How do you justify the difference in accuracy observed in the two experiments?\n",
    "\n",
    "The MLP model started slower than the Logistic Regression model with Cross Entropy loss however the accuracy of the MLP is higher at the end of the training. This is because the multilayer percepteron has a hidden layer that is better able to discover indirect corellations between the input and output values but since there are more values to train it take longer to ramp up.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
