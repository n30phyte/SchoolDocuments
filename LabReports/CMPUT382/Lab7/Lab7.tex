\RequirePackage[l2tabu, orthodox]{nag}
\documentclass{article}

\usepackage[letterpaper]{geometry}
\usepackage{booktabs}
\usepackage[final]{pdfpages}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{minted}

\usemintedstyle{xcode}
\setminted{fontsize=\footnotesize, linenos=true}

\title{CMPUT 382 Lab 7}
\author{Michael Kwok}
\begin{document}
\maketitle
\section{Floating point operations}
Line 10 starts off with \(\left(numInputs - blockSize\right)\) floating point operations.
Lines 16--19 which contain a single floating point add only gets executed \(\log\left(\frac{blockSize}{2}\right)\) times.

So in total, a reduction call does \( \left(numInputs - blockSize\right) + \log\left(\frac{blockSize}{2}\right) \) floating point operations.
\section{Global memory reads}
Line 7 does a global memory read \(numInputs\) times.
Line 10 does a global memory read \(\left(numInputs - blockSize\right)\) times.

Hence, the kernel call reads from global memory \( 2 \cdot numInputs - blockSize \) times.
\section{Global memory writes}
Global memory is only written to 1 time on Line 24.
\section{Synchronizations}
A single thread block synchronizes \(\log\left(\frac{blockSize}{2}\right) + 1\) time per chunk.
\newpage
\appendix
\section{Kernel code}
\inputminted{cuda}{test.cu}
\end{document}
